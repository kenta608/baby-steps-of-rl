{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day4 強化学習に対するニューラルネットワークの適用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化学習でNNを扱う際の実装フレームワーク\n",
    "code 4-7  NNを使った強化学習のフレームワークの定義 ~ Agent ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import re\n",
    "from collections import namedtuple, deque\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python import keras as K\n",
    "from PIL import Image\n",
    "\n",
    "# s: 状態, a: 行動, r: 報酬, n_s: 遷移先の状態, d: エピソード終了フラグ\n",
    "Experience = namedtuple(\"Experience\", [\"s\", \"a\", \"r\", \"n_s\", \"d\"])\n",
    "\n",
    "class FNAgent():\n",
    "\n",
    "    def __init__(self, epsilon, actions):\n",
    "        self.epsilon = epsilon\n",
    "        self.actions = actions\n",
    "        self.model = None\n",
    "        self.estimate_probs = False\n",
    "        self.initialized = False\n",
    "\n",
    "    # 学習したagentの保存\n",
    "    def save(self, model_path):\n",
    "        self.model.save(model_path, overwrite=True, include_optimizer=False)\n",
    "\n",
    "    # 学習したagentの読み込み\n",
    "    # クラス自体から呼び出せる\n",
    "    # (ex) クラスAが定義されており、hogeというクラスメソッドがある場合、A.hogeでアクセスできる (インスタンス化しなくて良い)\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        agent = cls(epsilon, actions)\n",
    "        agent.model = K.models.load_model(model_path)\n",
    "        agent.initialized = True\n",
    "        return agent\n",
    "\n",
    "    # パラメータを持つ関数の初期化\n",
    "    # 実装は継承先のクラスに委任\n",
    "    def initialize(self, experience):\n",
    "        raise NotImplementedError(\"You have to implement initialize method.\")\n",
    "\n",
    "    # 関数による予測\n",
    "    # 実装は継承先のクラスに委任\n",
    "    def estimate(self, s):\n",
    "        raise NotImplementedError(\"You have to implement estimate method.\")\n",
    "\n",
    "    # パラメータの更新\n",
    "    # 実装は継承先のクラスに委任\n",
    "    def update(self, experiences, gamma):\n",
    "        raise NotImplementedError(\"You have to implement update method.\")\n",
    "\n",
    "    # epsilon_greedy法\n",
    "    def policy(self, s):\n",
    "        if np.random.random() < self.epsilon or not self.initialized:\n",
    "            return np.random.randint(len(self.actions))\n",
    "        else:\n",
    "            estimates = self.estimate(s)\n",
    "            if self.estimate_probs:\n",
    "                action = np.random.choice(self.actions, size=1, p=estimates)[0]\n",
    "                return action\n",
    "            else:\n",
    "                return np.argmax(estimates)\n",
    "\n",
    "\n",
    "    def play(self, env, episode_count=5, render=True):\n",
    "        for e in range(episode_count):\n",
    "            s = env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                a = self.policy(s)\n",
    "                n_state, reward, done, info = env.step(a)\n",
    "                episode_reward += reward\n",
    "                s = n_state\n",
    "            else:\n",
    "                print(\"Get reward {}.\".format(episode_reward))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code 4-8 NNを使った強化学習のフレームワークの定義 ~ Trainer ~\n",
    "* Experience Replay (行動履歴を保存し、それを学習データに使用する)\n",
    "  * Agentの行動と学習は独立"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer():\n",
    "\n",
    "    def __init__(self, buffer_size=1024, batch_size=32, gamma=0.9, report_interval=10, log_dir=\"\"):\n",
    "        self.buffer_size = buffer_size # experiencesの大きさ\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.report_interval = report_interval\n",
    "        self.logger = Logger(log_dir, self.trainer_name)\n",
    "        self.experiences = deque(maxlen=buffer_size) # Agentの行動履歴 dequeがfullなら古い方から捨てる\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "\n",
    "    # 要解明\n",
    "    # property: hoge.trainer_nameで呼び出せるように\n",
    "    @property\n",
    "    def trainer_name(self):\n",
    "        class_name = self.__class__.__name__\n",
    "        snaked = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", class_name)\n",
    "        snaked = re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", snaked).lower()\n",
    "        snaked = snaked.replace(\"_trainer\", \"\")\n",
    "        return snaked\n",
    "\n",
    "    # 学習を行う\n",
    "    def train_loop(self, env, agent, episode=200, initial_count=-1,\n",
    "                   render=False, observe_interval=0):\n",
    "        self.experiences = deque(maxlen=self.buffer_size)\n",
    "        self.training = False\n",
    "        self.training_count = 0\n",
    "        self.reward_log = []\n",
    "        frames = []\n",
    "\n",
    "        for i in range(episode):\n",
    "            s = env.reset()\n",
    "            done = False\n",
    "            step_count = 0\n",
    "            self.episode_begin(i, agent)\n",
    "            while not done:\n",
    "                if render:\n",
    "                    env.render()\n",
    "                if self.training and observe_interval > 0 and\\\n",
    "                   (self.training_count == 1 or\n",
    "                    self.training_count % observe_interval == 0):\n",
    "                    frames.append(s) # observe_intervalの頻度で状態を保存\n",
    "\n",
    "                a = agent.policy(s)\n",
    "                n_state, reward, done, info = env.step(a)\n",
    "                e = Experience(s, a, reward, n_state, done)\n",
    "                self.experiences.append(e)\n",
    "                # フラグを立てる\n",
    "                if not self.training and \\\n",
    "                   len(self.experiences) == self.buffer_size:\n",
    "                    self.begin_train(i, agent)\n",
    "                    self.training = True\n",
    "\n",
    "                self.step(i, step_count, agent, e)\n",
    "\n",
    "                s = n_state\n",
    "                step_count += 1\n",
    "            else:\n",
    "                self.episode_end(i, step_count, agent)\n",
    "                # フラグを立てる\n",
    "                if not self.training and \\\n",
    "                   initial_count > 0 and i >= initial_count:\n",
    "                    self.begin_train(i, agent)\n",
    "                    self.training = True\n",
    "                # frameの内容をloggerに書き出す\n",
    "                if self.training:\n",
    "                    if len(frames) > 0:\n",
    "                        self.logger.write_image(self.training_count,\n",
    "                                                frames)\n",
    "                        frames = []\n",
    "                    self.training_count += 1\n",
    "\n",
    "    def episode_begin(self, episode, agent):\n",
    "        pass\n",
    "\n",
    "    def begin_train(self, episode, agent):\n",
    "        pass\n",
    "\n",
    "    def step(self, episode, step_count, agent, experience):\n",
    "        pass\n",
    "\n",
    "    def episode_end(self, episode, step_count, agent):\n",
    "        pass\n",
    "\n",
    "    def is_event(self, count, interval):\n",
    "        return True if count != 0 and count % interval == 0 else False\n",
    "\n",
    "    def get_recent(self, count):\n",
    "        \"\"\"最も現在からcount数分の経験をとる\"\"\"\n",
    "        recent = range(len(self.experiences) - count, len(self.experiences))\n",
    "        return [self.experiences[i] for i in recent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code 4-9 NNを使った強化学習のフレームワークの定義 ~ Observer ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Observer():\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self._env = env\n",
    "\n",
    "    @property\n",
    "    def action_space(self):\n",
    "        return self._env.action_space\n",
    "\n",
    "    @property\n",
    "    def observation_space(self):\n",
    "        return self._env.oservation_space\n",
    "\n",
    "    def reset(self):\n",
    "        return self.transform(self._env.reset())\n",
    "\n",
    "    def render(self):\n",
    "        self._env.render(mode=\"human\")\n",
    "\n",
    "    def step(self, action):\n",
    "        n_state, reward, done, info = self._env.step(action)\n",
    "        return self.transform(n_state), reward, done, info\n",
    "\n",
    "    def transform(self, state):\n",
    "        raise NotImplementedError(\"You have to implement transform method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code 4-10 NNを使った強化学習のフレームワークの定義 ~ Logger ~\n",
    "* コードの理解のためにtensorboardについて調べる必要ありそう"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger():\n",
    "\n",
    "    def __init__(self, log_dir=\"\", dir_name=\"\"):\n",
    "        self.log_dir = log_dir\n",
    "        if not log_dir:\n",
    "            self.log_dir = os.path.join(os.path.dirname(\"../log/\"), \"logs\")\n",
    "        if not os.path.exists(self.log_dir):\n",
    "            os.mkdir(self.log_dir)\n",
    "\n",
    "        if dir_name:\n",
    "            self.log_dir = os.path.join(self.log_dir, dir_name)\n",
    "            if not os.path.exists(self.log_dir):\n",
    "                os.mkdir(self.log_dir)\n",
    "\n",
    "        self._callback = tf.compat.v1.keras.callbacks.TensorBoard(\n",
    "                            self.log_dir)\n",
    "\n",
    "    @property\n",
    "    def writer(self):\n",
    "        return self._callback.writer\n",
    "\n",
    "    def set_model(self, model):\n",
    "        self._callback.set_model(model)\n",
    "\n",
    "    def path_of(self, file_name):\n",
    "        return os.path.join(self.log_dir, file_name)\n",
    "\n",
    "    def describe(self, name, values, episode=-1, step=-1):\n",
    "        mean = np.round(np.mean(values), 3)\n",
    "        std = np.round(np.std(values), 3)\n",
    "        desc = \"{} is {} (+/-{})\".format(name, mean, std)\n",
    "        if episode > 0:\n",
    "            print(\"At episode {}, {}\".format(episode, desc))\n",
    "        elif step > 0:\n",
    "            print(\"At step {}, {}\".format(step, desc))\n",
    "\n",
    "    def plot(self, name, values, interval=10):\n",
    "        indices = list(range(0, len(values), interval))\n",
    "        means = []\n",
    "        stds = []\n",
    "        for i in indices:\n",
    "            _values = values[i:(i + interval)]\n",
    "            means.append(np.mean(_values))\n",
    "            stds.append(np.std(_values))\n",
    "        means = np.array(means)\n",
    "        stds = np.array(stds)\n",
    "        plt.figure()\n",
    "        plt.title(\"{} History\".format(name))\n",
    "        plt.grid()\n",
    "        plt.fill_between(indices, means - stds, means + stds,\n",
    "                         alpha=0.1, color=\"g\")\n",
    "        plt.plot(indices, means, \"o-\", color=\"g\",\n",
    "                 label=\"{} per {} episode\".format(name.lower(), interval))\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.show()\n",
    "\n",
    "    def write(self, index, name, value):\n",
    "        summary = tf.compat.v1.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.tag = name\n",
    "        summary_value.simple_value = value\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()\n",
    "\n",
    "    def write_image(self, index, frames):\n",
    "        # Deal with a 'frames' as a list of sequential gray scaled image.\n",
    "        last_frames = [f[:, :, -1] for f in frames]\n",
    "        if np.min(last_frames[-1]) < 0:\n",
    "            scale = 127 / np.abs(last_frames[-1]).max()\n",
    "            offset = 128\n",
    "        else:\n",
    "            scale = 255 / np.max(last_frames[-1])\n",
    "            offset = 0\n",
    "        channel = 1  # gray scale\n",
    "        tag = \"frames_at_training_{}\".format(index)\n",
    "        values = []\n",
    "\n",
    "        for f in last_frames:\n",
    "            height, width = f.shape\n",
    "            array = np.asarray(f * scale + offset, dtype=np.uint8)\n",
    "            image = Image.fromarray(array)\n",
    "            output = io.BytesIO()\n",
    "            image.save(output, format=\"PNG\")\n",
    "            image_string = output.getvalue()\n",
    "            output.close()\n",
    "            image = tf.compat.v1.Summary.Image(\n",
    "                        height=height, width=width, colorspace=channel,\n",
    "                        encoded_image_string=image_string)\n",
    "            value = tf.compat.v1.Summary.Value(tag=tag, image=image)\n",
    "            values.append(value)\n",
    "\n",
    "        summary = tf.compat.v1.Summary(value=values)\n",
    "        self.writer.add_summary(summary, index)\n",
    "        self.writer.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Valueベースの強化学習をNNで実装する\n",
    "* 価値関数をNNで近似する\n",
    "\n",
    "### 題材 cart pole\n",
    "* 一本の縦に立てたポールを倒れないようにカートを位置を調整する\n",
    "* 状態\n",
    "  * カートの位置・加速度\n",
    "  * ポールの角度・倒れる速度(角速度)\n",
    "* 行動\n",
    "  * カートを右・左どちらに動かすか (どのくらいの速さでといった速度は？)\n",
    "* 報酬\n",
    "  * ポールが倒れていない -> 1 (常に1が単位時間で得られ続けて増えていくイメージ)\n",
    "* ポールが倒れた時点でエピソード終了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code4-11 ~ value function agent 実装 ~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import argparse\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "import gym\n",
    "\n",
    "class ValueFunctionAgent(FNAgent):\n",
    "\n",
    "    def save(self, model_path):\n",
    "        \"\"\"学習済みモデルの保存\"\"\"\n",
    "        joblib.dump(self.model, model_path)\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, env, model_path, epsilon=0.0001):\n",
    "        \"\"\"agentの読み込み\"\"\"\n",
    "        actions = list(range(env.action_space.n))\n",
    "        agent = cls(epsilon, actions)\n",
    "        agent.model = joblib.load(model_path)\n",
    "        agent.initialized = True\n",
    "        return agent\n",
    "\n",
    "    def initialize(self, experiences):\n",
    "        \"\"\"学習モデル、状態の初期化\"\"\"\n",
    "        scaler = StandardScaler()\n",
    "        estimator = MLPRegressor(hidden_layer_sizes=(10, 10), max_iter=1) # ノード数が10の隠れ層が2つ\n",
    "        self.model = Pipeline([(\"scaler\", scaler), (\"estimator\", estimator)])\n",
    "\n",
    "        states = np.vstack([e.s for e in experiences])\n",
    "        self.model.named_steps[\"scaler\"].fit(states) # 状態の正規化\n",
    "\n",
    "        # Avoid the predict before fit\n",
    "        self.update([experiences[0]], gamma=0)\n",
    "        self.initialized = True\n",
    "        print(\"Done initialization. From now, begin training!\")\n",
    "\n",
    "    def estimate(self, s):\n",
    "        \"\"\"学習モデルでの予測\"\"\"\n",
    "        estimated = self.model.predict(s)[0]\n",
    "        return estimated\n",
    "\n",
    "    def _predict(self, states):\n",
    "        \"\"\"\n",
    "        その状態における行動の価値を返す\n",
    "        学習前の場合はランダムな値を返す(initialized=Trueになっている時は学習後?)\n",
    "        \"\"\"\n",
    "        if self.initialized:\n",
    "            predicteds = self.model.predict(states)\n",
    "        else:\n",
    "            size = len(self.actions) * len(states)\n",
    "            predicteds = np.random.uniform(size=size)\n",
    "            predicteds = predicteds.reshape((-1, len(self.actions)))\n",
    "        return predicteds\n",
    "\n",
    "    def update(self, experiences, gamma):\n",
    "        \"\"\"モデルの学習\"\"\"\n",
    "        states = np.vstack([e.s for e in experiences])\n",
    "        n_states = np.vstack([e.n_s for e in experiences])\n",
    "\n",
    "        estimateds = self._predict(states)\n",
    "        future = self._predict(n_states)\n",
    "\n",
    "        # 実際にとった行動についての価値更新\n",
    "        # memo: e.dはbool型でエピソードが終了しているかどうかをみる\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.r\n",
    "            if not e.d:\n",
    "                reward += gamma * np.max(future[i]) # TD誤差\n",
    "            estimateds[i][e.a] = reward\n",
    "\n",
    "        # Q-tableのように縦に状態、横に行動をとり値は価値を格納したもの\n",
    "        estimateds = np.array(estimateds)\n",
    "        # ここ、pipelineにする必要あったのか？ (バラして使ってよかったのでは？)\n",
    "        states = self.model.named_steps[\"scaler\"].transform(states)\n",
    "        self.model.named_steps[\"estimator\"].partial_fit(states, estimateds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code4-12 CartPole環境のためのObserver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleObserver(Observer):\n",
    "\n",
    "    def transform(self, state):\n",
    "        return np.array(state).reshape((1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code4-13 価値関数の学習を行うtrainerを定義する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueFunctionTrainer(Trainer):\n",
    "\n",
    "    def train(self, env, episode_count=220, epsilon=0.1, initial_count=-1, render=False):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        agent = ValueFunctionAgent(epsilon, actions)\n",
    "        self.train_loop(env, agent, episode_count, initial_count, render)\n",
    "        return agent\n",
    "\n",
    "    def begin_train(self, episode, agent):\n",
    "        agent.initialize(self.experiences)\n",
    "\n",
    "    def step(self, episode, step_count, agent, experience):\n",
    "        if self.training:\n",
    "            batch = random.sample(self.experiences, self.batch_size)\n",
    "            agent.update(batch, self.gamma)\n",
    "\n",
    "    def episode_end(self, episode, step_count, agent):\n",
    "        rewards = [e.r for e in self.get_recent(step_count)] # step_count個の経験に対する価値\n",
    "        self.reward_log.append(sum(rewards))\n",
    "\n",
    "        if self.is_event(episode, self.report_interval):\n",
    "            recent_rewards = self.reward_log[-self.report_interval:]\n",
    "            self.logger.describe(\"reward\", recent_rewards, episode=episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code4-14 学習を実行するための処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(play):\n",
    "    env = CartPoleObserver(gym.make(\"CartPole-v0\"))\n",
    "    trainer = ValueFunctionTrainer()\n",
    "    path = trainer.logger.path_of(\"value_function_agent.pkl\")\n",
    "\n",
    "    if play:\n",
    "        agent = ValueFunctionAgent.load(env, path)\n",
    "        agent.play(env)\n",
    "    else:\n",
    "        trained = trainer.train(env)\n",
    "        trainer.logger.plot(\"Rewards\", trainer.reward_log, trainer.report_interval)\n",
    "\n",
    "        trained.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-788ca89733b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-0fbd3b6c3ba2>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(play)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mValueFunctionAgent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mtrained\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-386fd25e09bf>\u001b[0m in \u001b[0;36mplay\u001b[0;34m(self, env, episode_count, render)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mn_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-48e49a276f8f>\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl-book/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl-book/lib/python3.6/site-packages/gym/envs/classic_control/cartpole.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassic_control\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mViewer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreen_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscreen_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartwidth\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcartheight\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/rl-book/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpyglet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgl\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     raise ImportError('''\n",
      "\u001b[0;32m~/miniconda3/envs/rl-book/lib/python3.6/site-packages/pyglet/gl/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcarbon\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCarbonConfig\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m \u001b[0;32mdel\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;31m# XXX remove\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base' is not defined"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"VF Agent\")\n",
    "parser.add_argument(\"--play\", action=\"store_true\", help=\"play with trained model\")\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "args.play = True\n",
    "main(args.play)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37833f05d300d4440df5731c894ed8626520b175df0a9a96b8ab9ec7da84a4ab"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 ('rl-book')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
