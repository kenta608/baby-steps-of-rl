{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day4 強化学習に対するニューラルネットワークの適用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQN ~ CNNを使う ~\n",
    "\n",
    "* DQNのキモ?\n",
    "  * Experience Replay: 行動履歴を保存し、さまざまなエピソードから異なる時点の状態を同じミニバッチの学習データに使用する\n",
    "  * Fixed Target Q-Network: 学習最中のモデルから遷移先の価値を計算するのではなく、一定期間固定されたパラメータから算出する\n",
    "  * Clipping: 報酬を全ゲームを通じて、成功を1, 失敗を-1に統一する\n",
    "\n",
    "* CNNは6層\n",
    "  * conv:    8×8のフィルタを32枚, ストライド4, paddingは入力画像と出力画像が同じになるように, 活性化関数 ReLU\n",
    "  * conv:    4×4のフィルタを64枚, ストライド2, paddingは入力画像と出力画像が同じになるように, 活性化関数 ReLU\n",
    "  * conv:    3×3のフィルタを64枚, ストライド2`, paddingは入力画像と出力画像が同じになるように, 活性化関数 ReLU\n",
    "  * flatten: 画像の形をしたデータを１列のベクトルにする\n",
    "  * Dense:   出力するベクトルの次元を256次元に, 活性化関数 ReLU\n",
    "  * Dense:   出力するベクトルの次元は行動の数と同じに"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tensorflowでgpuを使う場合\n",
    "一度, tensorflowはuninstallし、tensorflow-gpuをinstallする\n",
    "\n",
    "```sh\n",
    "$ pip uninstall tensorflow tensorflow-estimator\n",
    "$ pip install tensorflow-gpu==1.14.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 11:33:04.708758: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
      "2022-11-16 11:33:04.750807: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2100000000 Hz\n",
      "2022-11-16 11:33:04.755093: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a47c8e5f60 executing computations on platform Host. Devices:\n",
      "2022-11-16 11:33:04.755118: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n",
      "2022-11-16 11:33:04.757134: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcuda.so.1\n",
      "2022-11-16 11:33:05.203358: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55a47eb9d300 executing computations on platform CUDA. Devices:\n",
      "2022-11-16 11:33:05.203389: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2022-11-16 11:33:05.203393: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (1): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2022-11-16 11:33:05.203396: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (2): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2022-11-16 11:33:05.203399: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (3): Tesla V100-PCIE-32GB, Compute Capability 7.0\n",
      "2022-11-16 11:33:05.215512: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 0 with properties: \n",
      "name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\n",
      "pciBusID: 0000:3d:00.0\n",
      "2022-11-16 11:33:05.216439: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 1 with properties: \n",
      "name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\n",
      "pciBusID: 0000:3e:00.0\n",
      "2022-11-16 11:33:05.217427: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 2 with properties: \n",
      "name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\n",
      "pciBusID: 0000:88:00.0\n",
      "2022-11-16 11:33:05.218407: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1640] Found device 3 with properties: \n",
      "name: Tesla V100-PCIE-32GB major: 7 minor: 0 memoryClockRate(GHz): 1.38\n",
      "pciBusID: 0000:89:00.0\n",
      "2022-11-16 11:33:05.218552: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcudart.so.10.0'; dlerror: libcudart.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-11-16 11:33:05.218619: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcublas.so.10.0'; dlerror: libcublas.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-11-16 11:33:05.218675: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcufft.so.10.0'; dlerror: libcufft.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-11-16 11:33:05.218723: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcurand.so.10.0'; dlerror: libcurand.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-11-16 11:33:05.218817: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusolver.so.10.0'; dlerror: libcusolver.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-11-16 11:33:05.218881: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Could not dlopen library 'libcusparse.so.10.0'; dlerror: libcusparse.so.10.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:\n",
      "2022-11-16 11:33:05.222404: I tensorflow/stream_executor/platform/default/dso_loader.cc:42] Successfully opened dynamic library libcudnn.so.7\n",
      "2022-11-16 11:33:05.222423: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1663] Cannot dlopen some GPU libraries. Skipping registering GPU devices...\n",
      "2022-11-16 11:33:05.222455: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1181] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2022-11-16 11:33:05.222462: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1187]      0 1 2 3 \n",
      "2022-11-16 11:33:05.222466: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 0:   N Y Y Y \n",
      "2022-11-16 11:33:05.222469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 1:   Y N Y Y \n",
      "2022-11-16 11:33:05.222471: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 2:   Y Y N Y \n",
      "2022-11-16 11:33:05.222474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1200] 3:   Y Y Y N \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 6708173589168198501,\n",
       " name: \"/device:XLA_CPU:0\"\n",
       " device_type: \"XLA_CPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 1972572433080393565\n",
       " physical_device_desc: \"device: XLA_CPU device\",\n",
       " name: \"/device:XLA_GPU:0\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 4716525398441322657\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:XLA_GPU:1\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 3992034414864283460\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:XLA_GPU:2\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 9670817580364291828\n",
       " physical_device_desc: \"device: XLA_GPU device\",\n",
       " name: \"/device:XLA_GPU:3\"\n",
       " device_type: \"XLA_GPU\"\n",
       " memory_limit: 17179869184\n",
       " locality {\n",
       " }\n",
       " incarnation: 5841414615415302248\n",
       " physical_device_desc: \"device: XLA_GPU device\"]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TensorFlowがGPUを認識しているか確認\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto(\n",
    "    gpu_options=tf.GPUOptions(\n",
    "        visible_device_list=\"3\", # specify GPU number\n",
    "        allow_growth=True\n",
    "    )\n",
    ")\n",
    "sess = tf.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "couldn't import doomish\n",
      "Couldn't import doom\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import argparse\n",
    "from collections import deque\n",
    "import numpy as np\n",
    "from tensorflow.python import keras as K\n",
    "from PIL import Image\n",
    "\n",
    "import gym\n",
    "import gym_ple\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../FN\")\n",
    "from fn_framework import FNAgent, Trainer, Observer\n",
    "\n",
    "\n",
    "class DeepQNetworkAgent(FNAgent):\n",
    "\n",
    "    def __init__(self, epsilon, actions):\n",
    "        super().__init__(epsilon, actions)\n",
    "        self._scaler = None\n",
    "        self._teacher_model = None\n",
    "\n",
    "    def initialize(self, experiences, optimizer):\n",
    "        \"\"\"モデルの構築\"\"\"\n",
    "        # 入力の形状取得(今回であれば画像のサイズ?)\n",
    "        feature_shape = experiences[0].s.shape\n",
    "        self.make_model(feature_shape)\n",
    "        self.model.compile(optimizer, loss=\"mse\")\n",
    "        self.initialized = True\n",
    "        print(\"Done initialization. From now, begin training!\")\n",
    "\n",
    "    def make_model(self, feature_shape):\n",
    "        \"\"\"モデルの作成\"\"\"\n",
    "        normal = K.initializers.glorot_normal()\n",
    "        model = K.Sequential()\n",
    "        model.add(K.layers.Conv2D(\n",
    "            32, kernel_size=8, strides=4, padding=\"same\",\n",
    "            input_shape=feature_shape, kernel_initializer=normal,\n",
    "            activation=\"relu\"))\n",
    "        model.add(K.layers.Conv2D(\n",
    "            64, kernel_size=4, strides=2, padding=\"same\",\n",
    "            kernel_initializer=normal,\n",
    "            activation=\"relu\"))\n",
    "        model.add(K.layers.Conv2D(\n",
    "            64, kernel_size=3, strides=1, padding=\"same\",\n",
    "            kernel_initializer=normal,\n",
    "            activation=\"relu\"))\n",
    "        model.add(K.layers.Flatten())\n",
    "        model.add(K.layers.Dense(256, kernel_initializer=normal,\n",
    "                                 activation=\"relu\"))\n",
    "        model.add(K.layers.Dense(len(self.actions),\n",
    "                                 kernel_initializer=normal))\n",
    "        self.model = model\n",
    "        self._teacher_model = K.models.clone_model(self.model)\n",
    "\n",
    "    def estimate(self, state):\n",
    "        return self.model.predict(np.array([state]))[0]\n",
    "\n",
    "    def update(self, experiences, gamma):\n",
    "        states = np.array([e.s for e in experiences])\n",
    "        n_states = np.array([e.n_s for e in experiences])\n",
    "\n",
    "        estimateds = self.model.predict(states)\n",
    "        # MLPの時とは違い、一定期間固定されたDQNから計算\n",
    "        # 学習の安定につながる\n",
    "        future = self._teacher_model.predict(n_states)\n",
    "\n",
    "        for i, e in enumerate(experiences):\n",
    "            reward = e.r\n",
    "            if not e.d:\n",
    "                reward += gamma * np.max(future[i])\n",
    "            estimateds[i][e.a] = reward\n",
    "\n",
    "        loss = self.model.train_on_batch(states, estimateds)\n",
    "        return loss\n",
    "    # Networkの更新\n",
    "    def update_teacher(self):\n",
    "        self._teacher_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code 4-16 CartPoleでテストするためのクラス  \n",
    "学習のプロセスが入っているmake_modelは時間がかかるため、それ以外の部分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetworkAgentTest(DeepQNetworkAgent):\n",
    "\n",
    "    def __init__(self, epsilon, actions):\n",
    "        super().__init__(epsilon, actions)\n",
    "\n",
    "    def make_model(self, feature_shape):\n",
    "        normal = K.initializers.glorot_normal()\n",
    "        model = K.Sequential()\n",
    "        model.add(K.layers.Dense(64, input_shape=feature_shape, kernel_initializer=normal, activation=\"relu\"))\n",
    "        model.add(K.layers.Dense(len(self.actions), kernel_initializer=normal, activation=\"relu\"))\n",
    "        self.model = model\n",
    "        self._teacher_model = K.models.clone_model(self.model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code 4-17 Observer定義  \n",
    "\n",
    "時系列に並んだ4つの画面フレームを入力とするため、4フレームをまとめる処理を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatcherObserver(Observer):\n",
    "\n",
    "    def __init__(self, env, width, height, frame_count):\n",
    "        super().__init__(env)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.frame_count = frame_count\n",
    "        self._frames = deque(maxlen=frame_count)\n",
    "\n",
    "    def transform(self, state):\n",
    "        grayed = Image.fromarray(state).convert(\"L\")\n",
    "        resized = grayed.resize((self.width, self.height))\n",
    "        resized = np.array(resized).astype(\"float\")\n",
    "        normalized = resized / 255.0  # scale to 0~1\n",
    "        if len(self._frames) == 0:\n",
    "            for i in range(self.frame_count):\n",
    "                self._frames.append(normalized)\n",
    "        else:\n",
    "            self._frames.append(normalized)\n",
    "        feature = np.array(self._frames)\n",
    "        # Convert the feature shape (f, w, h) => (h, w, f).\n",
    "        feature = np.transpose(feature, (1, 2, 0))\n",
    "\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code4-16 CartPoleでテストするためのクラス定義\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetworkAgentTest(DeepQNetworkAgent):\n",
    "\n",
    "    def __init__(self, epsilon, actions):\n",
    "        super().__init__(epsilon, actions)\n",
    "\n",
    "    def make_model(self, feature_shape):\n",
    "        normal = K.initializers.glorot_normal()\n",
    "        model = K.Sequential()\n",
    "        model.add(K.layers.Dense(64, input_shape=feature_shape,\n",
    "                                 kernel_initializer=normal, activation=\"relu\"))\n",
    "        model.add(K.layers.Dense(len(self.actions), kernel_initializer=normal,\n",
    "                                 activation=\"relu\"))\n",
    "        self.model = model\n",
    "        self._teacher_model = K.models.clone_model(self.model) # Fixed Target Q-Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code4-17 Catcherゲームを扱うためのObserver定義  \n",
    "時系列に並んだ4つのフレームをまとめる処理を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatcherObserver(Observer):\n",
    "\n",
    "    def __init__(self, env, width, height, frame_count):\n",
    "        super().__init__(env)\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.frame_count = frame_count\n",
    "        self._frames = deque(maxlen=frame_count)\n",
    "\n",
    "    def transform(self, state):\n",
    "        grayed = Image.fromarray(state).convert(\"L\")\n",
    "        resized = grayed.resize((self.width, self.height))\n",
    "        resized = np.array(resized).astype(\"float\")\n",
    "        normalized = resized / 255.0  # scale to 0~1\n",
    "        # 最初は4フレーム揃わないので、最初のフレームを４つコピー\n",
    "        if len(self._frames) == 0:\n",
    "            for i in range(self.frame_count):\n",
    "                self._frames.append(normalized)\n",
    "        else:\n",
    "            self._frames.append(normalized)\n",
    "        feature = np.array(self._frames)\n",
    "        # Convert the feature shape (f, w, h) => (h, w, f).\n",
    "        feature = np.transpose(feature, (1, 2, 0))\n",
    "\n",
    "        return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code4-18 Trainerの定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetworkTrainer(Trainer):\n",
    "\n",
    "    def __init__(self, buffer_size=50000, batch_size=32,\n",
    "                 gamma=0.99, initial_epsilon=0.5, final_epsilon=1e-3,\n",
    "                 learning_rate=1e-3, teacher_update_freq=3, report_interval=10,\n",
    "                 log_dir=\"\", file_name=\"\"):\n",
    "        super().__init__(buffer_size, batch_size, gamma,\n",
    "                         report_interval, log_dir)\n",
    "        self.file_name = file_name if file_name else \"dqn_agent.h5\"\n",
    "        self.initial_epsilon = initial_epsilon\n",
    "        self.final_epsilon = final_epsilon\n",
    "        self.learning_rate = learning_rate\n",
    "        self.teacher_update_freq = teacher_update_freq\n",
    "        self.loss = 0\n",
    "        self.training_episode = 0\n",
    "        self._max_reward = -10\n",
    "\n",
    "    def train(self, env, episode_count=1200, initial_count=200,\n",
    "              test_mode=False, render=False, observe_interval=100):\n",
    "        actions = list(range(env.action_space.n))\n",
    "        if not test_mode:\n",
    "            agent = DeepQNetworkAgent(1.0, actions)\n",
    "        else:\n",
    "            agent = DeepQNetworkAgentTest(1.0, actions)\n",
    "            observe_interval = 0\n",
    "        self.training_episode = episode_count\n",
    "\n",
    "        self.train_loop(env, agent, episode_count, initial_count, render,\n",
    "                        observe_interval)\n",
    "        return agent\n",
    "\n",
    "    def episode_begin(self, episode, agent):\n",
    "        self.loss = 0\n",
    "\n",
    "    def begin_train(self, episode, agent):\n",
    "        optimizer = K.optimizers.Adam(lr=self.learning_rate, clipvalue=1.0)\n",
    "        agent.initialize(self.experiences, optimizer)\n",
    "        self.logger.set_model(agent.model)\n",
    "        # epsilonを減衰して、ランダムな行動をしないようにしている\n",
    "        agent.epsilon = self.initial_epsilon\n",
    "        self.training_episode -= episode\n",
    "\n",
    "    def step(self, episode, step_count, agent, experience):\n",
    "        if self.training:\n",
    "            batch = random.sample(self.experiences, self.batch_size)\n",
    "            self.loss += agent.update(batch, self.gamma)\n",
    "\n",
    "    def episode_end(self, episode, step_count, agent):\n",
    "        \"\"\"報酬や誤差(Loss)を記録し、学習途中でモデル保存\"\"\"\n",
    "        reward = sum([e.r for e in self.get_recent(step_count)])\n",
    "        self.loss = self.loss / step_count\n",
    "        self.reward_log.append(reward)\n",
    "        if self.training:\n",
    "            self.logger.write(self.training_count, \"loss\", self.loss)\n",
    "            self.logger.write(self.training_count, \"reward\", reward)\n",
    "            self.logger.write(self.training_count, \"epsilon\", agent.epsilon)\n",
    "            if reward > self._max_reward:\n",
    "                agent.save(self.logger.path_of(self.file_name))\n",
    "                self._max_reward = reward\n",
    "            if self.is_event(self.training_count, self.teacher_update_freq):\n",
    "                agent.update_teacher()\n",
    "\n",
    "            diff = (self.initial_epsilon - self.final_epsilon)\n",
    "            decay = diff / self.training_episode\n",
    "            agent.epsilon = max(agent.epsilon - decay, self.final_epsilon)\n",
    "\n",
    "        if self.is_event(episode, self.report_interval):\n",
    "            recent_rewards = self.reward_log[-self.report_interval:]\n",
    "            self.logger.describe(\"reward\", recent_rewards, episode=episode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code 4-19 実際に学習を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(play, is_test):\n",
    "    file_name = \"dqn_agent.h5\" if not is_test else \"dqn_agent_test.h5\"\n",
    "    trainer = DeepQNetworkTrainer(file_name=file_name)\n",
    "    path = trainer.logger.path_of(trainer.file_name)\n",
    "    agent_class = DeepQNetworkAgent\n",
    "\n",
    "    if is_test:\n",
    "        print(\"Train on test mode\")\n",
    "        obs = gym.make(\"CartPole-v0\")\n",
    "        agent_class = DeepQNetworkAgentTest\n",
    "    else:\n",
    "        env = gym.make(\"Catcher-v0\")\n",
    "        obs = CatcherObserver(env, 80, 80, 4)\n",
    "        trainer.learning_rate = 1e-4\n",
    "\n",
    "    if play:\n",
    "        agent = agent_class.load(obs, path)\n",
    "        agent.play(obs, render=True)\n",
    "    else:\n",
    "        trainer.train(obs, test_mode=is_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kenta_suzuki/miniconda3/envs/rl-book/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'gym_ple.ple_env.PLEEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "At episode 10, reward is -6.8 (+/-1.72)\n",
      "At episode 20, reward is -7.3 (+/-1.005)\n",
      "At episode 30, reward is -7.4 (+/-0.8)\n",
      "At episode 40, reward is -7.0 (+/-1.0)\n",
      "At episode 50, reward is -7.3 (+/-0.781)\n",
      "At episode 60, reward is -6.7 (+/-1.1)\n",
      "At episode 70, reward is -7.3 (+/-1.005)\n",
      "At episode 80, reward is -6.2 (+/-1.47)\n",
      "At episode 90, reward is -6.6 (+/-1.114)\n",
      "At episode 100, reward is -7.3 (+/-0.64)\n",
      "At episode 110, reward is -7.1 (+/-0.831)\n",
      "At episode 120, reward is -6.5 (+/-1.36)\n",
      "At episode 130, reward is -6.9 (+/-1.044)\n",
      "At episode 140, reward is -6.5 (+/-1.36)\n",
      "At episode 150, reward is -7.0 (+/-0.632)\n",
      "At episode 160, reward is -6.8 (+/-1.166)\n",
      "At episode 170, reward is -7.1 (+/-1.221)\n",
      "At episode 180, reward is -6.3 (+/-1.1)\n",
      "At episode 190, reward is -6.8 (+/-1.077)\n",
      "At episode 200, reward is -6.8 (+/-1.077)\n",
      "WARNING:tensorflow:From /home/kenta_suzuki/miniconda3/envs/rl-book/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1288: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/kenta_suzuki/miniconda3/envs/rl-book/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling GlorotNormal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/kenta_suzuki/miniconda3/envs/rl-book/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Done initialization. From now, begin training!\n",
      "At episode 210, reward is -7.3 (+/-0.9)\n",
      "At episode 220, reward is -6.3 (+/-1.847)\n",
      "At episode 230, reward is -5.3 (+/-1.345)\n",
      "At episode 240, reward is -4.4 (+/-2.905)\n",
      "At episode 250, reward is 6.6 (+/-10.022)\n",
      "At episode 260, reward is 19.6 (+/-15.035)\n",
      "At episode 270, reward is 11.1 (+/-10.793)\n",
      "At episode 280, reward is 11.5 (+/-8.902)\n",
      "At episode 290, reward is 8.9 (+/-7.7)\n",
      "At episode 300, reward is 6.9 (+/-8.96)\n",
      "At episode 310, reward is 5.3 (+/-9.078)\n",
      "At episode 320, reward is 4.1 (+/-9.994)\n",
      "At episode 330, reward is 4.4 (+/-7.486)\n",
      "At episode 340, reward is 13.4 (+/-11.526)\n",
      "At episode 350, reward is 12.6 (+/-18.2)\n",
      "At episode 360, reward is 1.1 (+/-6.549)\n",
      "At episode 370, reward is -2.0 (+/-3.347)\n",
      "At episode 380, reward is 3.3 (+/-7.253)\n",
      "At episode 390, reward is -0.4 (+/-4.779)\n",
      "At episode 400, reward is 2.3 (+/-4.961)\n",
      "At episode 410, reward is 1.8 (+/-5.134)\n",
      "At episode 420, reward is 4.4 (+/-5.122)\n",
      "At episode 430, reward is 12.0 (+/-7.603)\n",
      "At episode 440, reward is 7.3 (+/-9.644)\n",
      "At episode 450, reward is 6.0 (+/-10.218)\n",
      "At episode 460, reward is 5.3 (+/-8.9)\n",
      "At episode 470, reward is 2.7 (+/-6.051)\n",
      "At episode 480, reward is 8.6 (+/-6.844)\n",
      "At episode 490, reward is 0.9 (+/-5.147)\n",
      "At episode 500, reward is 5.4 (+/-7.459)\n",
      "At episode 510, reward is 5.7 (+/-7.349)\n",
      "At episode 520, reward is 4.3 (+/-6.149)\n",
      "At episode 530, reward is 3.9 (+/-7.752)\n",
      "At episode 540, reward is 8.1 (+/-7.382)\n",
      "At episode 550, reward is 17.7 (+/-26.027)\n",
      "At episode 560, reward is 5.9 (+/-5.941)\n",
      "At episode 570, reward is 2.9 (+/-5.7)\n",
      "At episode 580, reward is 6.1 (+/-9.596)\n",
      "At episode 590, reward is 6.3 (+/-5.292)\n",
      "At episode 600, reward is 12.5 (+/-12.428)\n",
      "At episode 610, reward is 10.1 (+/-11.406)\n",
      "At episode 620, reward is 3.2 (+/-7.743)\n",
      "At episode 630, reward is 7.4 (+/-6.003)\n",
      "At episode 640, reward is 10.8 (+/-11.374)\n",
      "At episode 650, reward is 8.8 (+/-6.6)\n",
      "At episode 660, reward is 14.6 (+/-10.385)\n",
      "At episode 670, reward is 19.9 (+/-17.507)\n",
      "At episode 680, reward is 19.5 (+/-17.5)\n",
      "At episode 690, reward is 10.9 (+/-11.743)\n",
      "At episode 700, reward is 10.4 (+/-11.2)\n",
      "At episode 710, reward is 15.0 (+/-17.844)\n",
      "At episode 720, reward is 13.5 (+/-7.593)\n",
      "At episode 730, reward is 8.9 (+/-11.318)\n",
      "At episode 740, reward is 9.4 (+/-13.735)\n",
      "At episode 750, reward is 2.9 (+/-8.166)\n",
      "At episode 760, reward is 9.8 (+/-12.311)\n",
      "At episode 770, reward is 16.3 (+/-11.807)\n",
      "At episode 780, reward is 21.3 (+/-19.241)\n",
      "At episode 790, reward is 26.8 (+/-20.139)\n",
      "At episode 800, reward is 22.1 (+/-12.629)\n",
      "At episode 810, reward is 25.0 (+/-20.313)\n",
      "At episode 820, reward is 17.4 (+/-12.347)\n",
      "At episode 830, reward is 18.0 (+/-9.91)\n",
      "At episode 840, reward is 12.7 (+/-7.001)\n",
      "At episode 850, reward is 22.6 (+/-26.402)\n",
      "At episode 860, reward is 44.0 (+/-27.713)\n",
      "At episode 870, reward is 41.3 (+/-24.29)\n",
      "At episode 880, reward is 42.2 (+/-27.312)\n",
      "At episode 890, reward is 61.3 (+/-35.573)\n",
      "At episode 900, reward is 32.8 (+/-46.0)\n",
      "At episode 910, reward is 49.4 (+/-37.866)\n",
      "At episode 920, reward is 26.0 (+/-21.913)\n",
      "At episode 930, reward is 13.5 (+/-11.935)\n",
      "At episode 940, reward is 30.2 (+/-32.588)\n",
      "At episode 950, reward is 40.1 (+/-34.393)\n",
      "At episode 960, reward is 78.2 (+/-43.377)\n",
      "At episode 970, reward is 46.0 (+/-32.83)\n",
      "At episode 980, reward is 62.3 (+/-40.677)\n",
      "At episode 990, reward is 48.4 (+/-33.263)\n",
      "At episode 1000, reward is 73.5 (+/-48.413)\n",
      "At episode 1010, reward is 48.6 (+/-32.626)\n",
      "At episode 1020, reward is 56.0 (+/-58.849)\n",
      "At episode 1030, reward is 91.7 (+/-41.166)\n",
      "At episode 1040, reward is 68.0 (+/-40.98)\n",
      "At episode 1050, reward is 64.8 (+/-55.717)\n",
      "At episode 1060, reward is 71.0 (+/-41.277)\n",
      "At episode 1070, reward is 80.8 (+/-63.111)\n",
      "At episode 1080, reward is 130.8 (+/-44.864)\n",
      "At episode 1090, reward is 87.4 (+/-55.599)\n",
      "At episode 1100, reward is 88.7 (+/-60.311)\n",
      "At episode 1110, reward is 69.5 (+/-39.373)\n",
      "At episode 1120, reward is 110.3 (+/-77.411)\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"DQN Agent\")\n",
    "parser.add_argument(\"--play\", action=\"store_true\",\n",
    "                    help=\"play with trained model\")\n",
    "parser.add_argument(\"--test\", action=\"store_true\",\n",
    "                    help=\"train by test mode\")\n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "main(args.play, args.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "37833f05d300d4440df5731c894ed8626520b175df0a9a96b8ab9ec7da84a4ab"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
